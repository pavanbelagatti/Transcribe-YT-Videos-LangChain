{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Installing libraries and connect to LLMs"
      ],
      "metadata": {
        "id": "eKmRchcKrL2D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6tBEwQfWJPr"
      },
      "outputs": [],
      "source": [
        "!pip install -qU  \\\n",
        "  python-dotenv \\\n",
        "  langchain \\\n",
        "  langchain-community \\\n",
        "  openai \\\n",
        "  langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Set your API keys as environment variables\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "F4qwmPHHXhD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_api_key = os.getenv('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "FnTY8UU7YGCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to OpenAI\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm_gpt4 = ChatOpenAI(model=\"gpt-4\")"
      ],
      "metadata": {
        "id": "njYjdeDuYHgu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that you can use the LLM\n",
        "llm_gpt4.invoke(\"What is a large language model?\").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "PFVRUMG-YRDx",
        "outputId": "9d20c766-bbd3-435e-ad8c-3fc51eafcbe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A large language model is an artificial intelligence model that has been trained on a vast amount of text data. It uses this data to generate human-like text based on the input it is given. These models, such as OpenAI\\'s GPT-3, are capable of completing tasks that require a deep understanding of language, like translation, answering questions, creating written content, summarization, and more. They are called \"large\" because they have a high number of parameters, often in the billions, allowing them to capture more information and produce more accurate results.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Prompt Engineering"
      ],
      "metadata": {
        "id": "ixR9D8LatBvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic request using system and human/user message\n",
        "\n",
        "system_prompt=\"\"\"\n",
        "You explain things to people like they are five year olds.\n",
        "\"\"\"\n",
        "user_prompt=f\"\"\"\n",
        "What is large language model?\n",
        "\"\"\"\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "import textwrap\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=system_prompt),\n",
        "    HumanMessage(content=user_prompt),\n",
        "]"
      ],
      "metadata": {
        "id": "ePx9Yt9DZU8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response=llm_gpt4.invoke(messages)\n",
        "answer = textwrap.fill(response.content, width=100)"
      ],
      "metadata": {
        "id": "mJGvRQmkZeg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OE9Uvj4SZhG0",
        "outputId": "23701599-a523-49b7-98e1-83f8639412da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, imagine you have a really big toy robot. This robot has been taught to understand and use\n",
            "human language. You can ask it questions or tell it to write a story, and it will try its best to do\n",
            "it. It learned how to do this by reading lots and lots of books, websites, and other stuff people\n",
            "wrote. We call this big language-knowing toy robot a \"large language model\". It's like a super smart\n",
            "parrot that can repeat things it has learned, but also try to make new sentences based on what it\n",
            "knows. However, just like a parrot, it doesn't really understand what it's saying, it's just really\n",
            "good at copying humans.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template from LangChain"
      ],
      "metadata": {
        "id": "3yS93F6rtTFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "9Kh_hWXYZqHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple prompt template\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following input:\n",
        "{topic}\n",
        "Provide an explanation of the given topic.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt from the prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "jQ0KwzU7ZtJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Composing the Chain"
      ],
      "metadata": {
        "id": "CotEhKPMtm5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble the chain using the pipe operator\n",
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "ptJTYjqcZw6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"topic\":\"What is large language model\"}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "vMbIBb3sZ0LI",
        "outputId": "95d6a69b-9e78-4d9c-c988-52238df65788"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A large language model is a type of artificial intelligence model that has been trained on a vast amount of text data. These models, like OpenAI\\'s GPT-3 or Google\\'s BERT, are designed to generate human-like text based on the input they are given.\\n\\nThe \"large\" in large language model refers to the size of the model in terms of the number of parameters it has. These models can have billions or even trillions of parameters, allowing them to capture a wide range of nuances in the data they were trained on.\\n\\nThe models are capable of understanding context, completing sentences, generating whole paragraphs, and even writing an essay on a given topic. Their applications span across various fields such as content creation, dialogue systems, translation, and more. However, they also have limitations and can sometimes produce outputs that are biased, nonsensical, or inappropriate.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain to Transcribe YouTube Videos"
      ],
      "metadata": {
        "id": "ts_RraFOt2G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade --quiet  youtube-transcript-api"
      ],
      "metadata": {
        "id": "8xCnAO0kZ5ZC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loaders from LangChain"
      ],
      "metadata": {
        "id": "aco6kwuquFyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Youtube Loader from the LangChain community\n",
        "\n",
        "from langchain_community.document_loaders import YoutubeLoader\n",
        "\n",
        "loader = YoutubeLoader.from_youtube_url(\n",
        "    \"https://youtu.be/h04DwdAkNZ4?si=C7MPK1mqvkBzUAAR\", add_video_info=False\n",
        ")"
      ],
      "metadata": {
        "id": "dLNlpxx7aGPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the video transcript as documents\n",
        "docs=loader.load()"
      ],
      "metadata": {
        "id": "nW71TEPPaLYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kql7z51QaOR4",
        "outputId": "2c398945-faec-494a-c28c-51352a85e259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"uh Hey guys so recently I have written this uh this article or you can call it a tutorial on how to create open source AI applications using Lang chain uh so we need to understand that uh LMS are not just enough to create your to build your eii applications you need to have a proper toolkit or a framework and uh um eii Frameworks like Lang chain and Lama andex uh uh help us build these a applications seamlessly uh so uh uh what is langin it's basically an open open source framework for eiml data Engineers to develop uh sophisticated Eid driven applications powered by llm and uh basically langin facilitates uh the integration of uh language models with all the required components including um external databases like vector databases logic reasoning apis and Etc so so these all are required to enance the capabilities of llm powered applications so Lan chain and then llama index they both provide uh provide this this toolkit so basically langin has six modules and U they include models chains uh prompts indexes memory and agents so all these six models um help us build uh llm powered applications uh seamlessly uh so in this tutorial what I've have done is um um uh we have used langin like uh like I said as a framework and then we have used publicly available uh PDF we have split the PDF uh and uh and and we have stored the chunks of the PDF uh into a vector database like single store um uh and uh we will ask the query at the end and retrieve the most relevant uh response uh for our query so uh we will use a single Stone notebook feature it's just like a Google collab or um uh you just like jupyter notebooks uh where uh you will build uh where we'll we'll run our application code and see how uh the things go forward okay so for that you need to uh activate your single store uh single store um account uh it's free for that you need to go to single store and then uh try for free it's completely free free forever no time bound and once you sign up you're going to get $600 worth free credits for the first time uh I already have my account so I'll just sign in um so let's go back to our article uh first things first you need to create a workspace uh for yourself and uh after creating a workspace you you need to create a database uh um uh let me show you so once you sign up you'll you you'll land on your single store dashboard like something like this if it's your first time it might uh look something little different for you because I have already I've been using this uh dashboard for uh uh for very long and I have created some notebooks already so if you go to deployments you can see um this is the this is this is my workspace I have already created and these are the databases attached to my workspace creating another workspace is very easy and this is how you can create a cloud workspace for yourself you just give it a name and then create a workspace I already have my have my workspace and then creating a database under your workspace is very easy just say create a database give it a name and then say Creator databas right so uh that's what we did and then we are going to use like I said uh single store notebooks uh where we will add all our notebook code and then we'll execute the code one by one to see how um how we can build our AI framework like so we should go to develop and once you go to this develop tab there is new notebook there is a notebook feature and uh you can say new notebook and then just name your notebook and then create a notebook you can create a blank notebook you can create a um there there's a personal notebook you can create and also you can create a shade notebook if you are working with your colleagues or somebody okay so once you create a notebook um this is how let me show you that to you I can say test create a notebook you will go to a dashboard of the notebook where you can add the code and then start just running can add the code and run the code okay so it's just uh spinning up yeah this is our this is how the notebook looks this is just for the example so you can add this code basically so what we are doing so basically we are installing the required libraries from lanin right we we are we are building this CI app using Lan chain so we need lch so we'll just copy this and add this here okay copy this thing then add it here and then we'll just run it so you will do the same thing next thing is load the PDF so we have this um publicly available PDF uh we will load that PDF and then what we'll do we'll split and uh read the content of the PDF we'll split that PDF into chunks and then what we'll do we'll set up a database to store the contents of a PDF okay so we are creating a database by name uh Lang DB and uh we are storing uh our content there and uh next thing is uh we need a open API key open API key and then we are we mentioning that and then we creating embeddings and inserting uh them into the database okay all the content along with the um vector embeddings and then we are asking a query like query text is um will object oriented database uh bases be commercially successful so this is a query um we are asking and it's going to give us the relevant answer and then we can also find the most similar text from the PDF okay and uh we are uh telling our we are using the model GPT 3.5 TBO and uh the role of this whole thing is like you know we are telling it like you are a helpful assistant so it'll act as our assistant okay and it'll for any query it's going to answer us back okay so that's how quickly you can build your AI application using uh L chain so notebook code is here uh you can just go to this uh I I'll share this notebook code to you in in the comments or in the video description and you can go and try this okay so yeah thank you\", metadata={'source': 'h04DwdAkNZ4'})]"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain to Summarise Youtube Video Giving a Transcript"
      ],
      "metadata": {
        "id": "M3vbzfLgvSk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transcript=docs[0].page_content"
      ],
      "metadata": {
        "id": "Sr0XPjCmaTYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now use the transcript in a chain\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains YT videos. Given the following video transcript:\n",
        "{video_transcript}\n",
        "Give a summary.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"video_transcript\"],\n",
        "    template=prompt_template,\n",
        ")"
      ],
      "metadata": {
        "id": "WLpmOPNwaWqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt | llm_gpt4"
      ],
      "metadata": {
        "id": "hT74ZLvYacBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that we can just feed the chain the docs without extracting the content as text\n",
        "\n",
        "chain.invoke({\"video_transcript\":docs}).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "5FfVgPq_aeu_",
        "outputId": "ff2c0049-4ec3-4324-c52d-ee45f3bf958b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The video is a tutorial on how to create open source AI applications using Lang chain. The speaker explains that Lang chain is an open-source framework for data engineers to develop AI-driven applications powered by language models. It facilitates the integration of language models with all required components, including external databases and logic reasoning APIs. The speaker demonstrates how to use Lang chain to split a publicly available PDF into chunks and store them into a vector database. They also show how to ask a query and retrieve the most relevant response. The speaker uses a Single Store notebook feature and mentions the need to create a workspace and a database. They provide a step-by-step guide on how to install the required libraries, load the PDF, split it into chunks, store the contents into a database, and ask a query.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain"
      ],
      "metadata": {
        "id": "RG2bruPMaktz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The create_stuff_documents_chain takes a list of docs and formats them all into a prompt\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "You are a helpful assistant that explains AI topics. Given the following context:\n",
        "{context}\n",
        "Summarize what LangChain can do.\n",
        "\"\"\"\n",
        "\n",
        "# Create the prompt\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "chain = create_stuff_documents_chain(llm_gpt4, prompt)"
      ],
      "metadata": {
        "id": "J5mU-6zRaphM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#docs"
      ],
      "metadata": {
        "id": "_qQTmv9Ma3oR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"context\": docs})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "1NqsTHNYa6GO",
        "outputId": "7633fdac-63a7-4bd1-e7b5-9ea49aaed33a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an open-source framework designed to aid data engineers in developing sophisticated AI-driven applications. It facilitates the integration of language models with various components, including external databases, logic reasoning APIs, and more to enhance the capabilities of AI applications. LangChain includes six modules - models, chains, prompts, indexes, memory, and agents - that aid in the seamless construction of AI applications. It allows users to load and split PDFs, set up databases to store content, create embeddings, and insert them into the database. Users can then ask queries and retrieve the most relevant response.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "summarize_prompt_template = \"\"\"\n",
        "You are a helpful assistant that summarizes AI concepts:\n",
        "{context}\n",
        "Summarize the context\n",
        "\"\"\"\n",
        "\n",
        "summarize_prompt = PromptTemplate.from_template(summarize_prompt_template)"
      ],
      "metadata": {
        "id": "WHEkwYHJbDc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarize_prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvFrF56ObGE1",
        "outputId": "7b5d4fab-f4aa-4c0d-d48e-ceafb8cdff24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context'], template='\\nYou are a helpful assistant that summarizes AI concepts:\\n{context}\\nSummarize the context\\n')"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser = StrOutputParser()\n",
        "\n",
        "chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "JNrDQkOebJ-Y",
        "outputId": "99acea52-96ad-41cd-f165-c8d7e58c1b72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'LangChain is an Artificial Intelligence (AI) project based on blockchain technology. Its primary aim is to develop a decentralized translation solution. The project utilizes AI and the power of community contributions to facilitate accurate and efficient translation services. The features of blockchain like transparency, security, and incentives (through tokens) are used to encourage contributors to improve the AI translation models. LangChain aims to disrupt the traditional translation industry by providing a more affordable, quicker, and reliable translation service.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the type of the chain\n",
        "print(type(chain)) # Should print <class 'langchain_core.runnables.base.RunnableSequence'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "550yeGsebNvM",
        "outputId": "fba01173-a4f8-4ea3-99b4-45f1d52df737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inject python functions into a chain with RunnableLambda\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "\n",
        "summarize_chain = summarize_prompt | llm_gpt4 | output_parser\n",
        "\n",
        "# Define a custom lambda function and wrap it in RunnableLambda\n",
        "length_lambda = RunnableLambda(lambda summary: f\"Summary length: {len(summary)} characters\")\n",
        "\n",
        "lambda_chain = summarize_chain | length_lambda\n",
        "\n",
        "lambda_chain.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jK6_gWMKbSph",
        "outputId": "dd365a8e-496b-4036-cf56-ffe506d4bab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 1429 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(lambda_chain.steps[-1])) # Should print <class 'langchain_core.runnables.base.RunnableLambda'>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3EM9_rXbWC0",
        "outputId": "0f6ac307-c3bb-4ba7-c25b-4395ce25333c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use function in chain without converting to RunnableLambda\n",
        "chain_with_function = summarize_chain |  (lambda summary: f\"Summary length: {len(summary)} characters\")"
      ],
      "metadata": {
        "id": "JY_uIZI1bZjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(chain_with_function.steps[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eX_J_-Hzbcfa",
        "outputId": "77e548d4-3f29-4d3c-9b21-b036f8e73975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.runnables.base.RunnableLambda'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain_with_function.invoke({\"context\": \"What is LangChain?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GjxnDXNpbfyn",
        "outputId": "69175cf8-56d1-4d45-8cc1-f93ab4ba8228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Summary length: 679 characters'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Splitters from LangChain (Chunking Data)"
      ],
      "metadata": {
        "id": "g8Qc2tVpwJvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter"
      ],
      "metadata": {
        "id": "ZhTYHkO3bsMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")"
      ],
      "metadata": {
        "id": "i13xZIMJb8q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "qggDxFotb_iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs_split"
      ],
      "metadata": {
        "id": "z-cgp_5NcCfF",
        "outputId": "b26a6d5d-fb9a-46de-8fcf-fa057192fe58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='uh Hey guys so recently I have written this uh this article or you can call it a tutorial on how to', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='tutorial on how to create open source AI applications using Lang chain uh so we need to understand', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='need to understand that uh LMS are not just enough to create your to build your eii applications', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='eii applications you need to have a proper toolkit or a framework and uh um eii Frameworks like', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='eii Frameworks like Lang chain and Lama andex uh uh help us build these a applications seamlessly', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"seamlessly uh so uh uh what is langin it's basically an open open source framework for eiml data\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='for eiml data Engineers to develop uh sophisticated Eid driven applications powered by llm and uh', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='by llm and uh basically langin facilitates uh the integration of uh language models with all the', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='models with all the required components including um external databases like vector databases logic', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='databases logic reasoning apis and Etc so so these all are required to enance the capabilities of', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='the capabilities of llm powered applications so Lan chain and then llama index they both provide uh', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='both provide uh provide this this toolkit so basically langin has six modules and U they include', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='and U they include models chains uh prompts indexes memory and agents so all these six models um', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='these six models um help us build uh llm powered applications uh seamlessly uh so in this tutorial', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"so in this tutorial what I've have done is um um uh we have used langin like uh like I said as a\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='uh like I said as a framework and then we have used publicly available uh PDF we have split the PDF', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='have split the PDF uh and uh and and we have stored the chunks of the PDF uh into a vector database', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='a vector database like single store um uh and uh we will ask the query at the end and retrieve the', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='and retrieve the most relevant uh response uh for our query so uh we will use a single Stone', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"use a single Stone notebook feature it's just like a Google collab or um uh you just like jupyter\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"just like jupyter notebooks uh where uh you will build uh where we'll we'll run our application\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='run our application code and see how uh the things go forward okay so for that you need to uh', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"that you need to uh activate your single store uh single store um account uh it's free for that you\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"free for that you need to go to single store and then uh try for free it's completely free free\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"free free forever no time bound and once you sign up you're going to get $600 worth free credits\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"worth free credits for the first time uh I already have my account so I'll just sign in um so let's\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"sign in um so let's go back to our article uh first things first you need to create a workspace uh\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='a workspace uh for yourself and uh after creating a workspace you you need to create a database uh', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"a database uh um uh let me show you so once you sign up you'll you you'll land on your single store\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"your single store dashboard like something like this if it's your first time it might uh look\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"it might uh look something little different for you because I have already I've been using this uh\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='been using this uh dashboard for uh uh for very long and I have created some notebooks already so', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='already so if you go to deployments you can see um this is the this is this is my workspace I have', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='my workspace I have already created and these are the databases attached to my workspace creating', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='workspace creating another workspace is very easy and this is how you can create a cloud workspace', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='a cloud workspace for yourself you just give it a name and then create a workspace I already have', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='I already have my have my workspace and then creating a database under your workspace is very easy', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='is very easy just say create a database give it a name and then say Creator databas right so uh', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"databas right so uh that's what we did and then we are going to use like I said uh single store\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"uh single store notebooks uh where we will add all our notebook code and then we'll execute the\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"we'll execute the code one by one to see how um how we can build our AI framework like so we should\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='like so we should go to develop and once you go to this develop tab there is new notebook there is', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='notebook there is a notebook feature and uh you can say new notebook and then just name your', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='then just name your notebook and then create a notebook you can create a blank notebook you can', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"notebook you can create a um there there's a personal notebook you can create and also you can\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='and also you can create a shade notebook if you are working with your colleagues or somebody okay', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='or somebody okay so once you create a notebook um this is how let me show you that to you I can say', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='to you I can say test create a notebook you will go to a dashboard of the notebook where you can', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='where you can add the code and then start just running can add the code and run the code okay so', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"the code okay so it's just uh spinning up yeah this is our this is how the notebook looks this is\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='looks this is just for the example so you can add this code basically so what we are doing so', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='we are doing so basically we are installing the required libraries from lanin right we we are we', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"right we we are we are building this CI app using Lan chain so we need lch so we'll just copy this\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"just copy this and add this here okay copy this thing then add it here and then we'll just run it\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"we'll just run it so you will do the same thing next thing is load the PDF so we have this um\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"so we have this um publicly available PDF uh we will load that PDF and then what we'll do we'll\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"what we'll do we'll split and uh read the content of the PDF we'll split that PDF into chunks and\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"PDF into chunks and then what we'll do we'll set up a database to store the contents of a PDF okay\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='of a PDF okay so we are creating a database by name uh Lang DB and uh we are storing uh our content', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='uh our content there and uh next thing is uh we need a open API key open API key and then we are we', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='and then we are we mentioning that and then we creating embeddings and inserting uh them into the', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='uh them into the database okay all the content along with the um vector embeddings and then we are', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='and then we are asking a query like query text is um will object oriented database uh bases be', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"uh bases be commercially successful so this is a query um we are asking and it's going to give us\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='going to give us the relevant answer and then we can also find the most similar text from the PDF', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='text from the PDF okay and uh we are uh telling our we are using the model GPT 3.5 TBO and uh the', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='3.5 TBO and uh the role of this whole thing is like you know we are telling it like you are a', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"it like you are a helpful assistant so it'll act as our assistant okay and it'll for any query it's\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"for any query it's going to answer us back okay so that's how quickly you can build your AI\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='can build your AI application using uh L chain so notebook code is here uh you can just go to this', metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content=\"can just go to this uh I I'll share this notebook code to you in in the comments or in the video\", metadata={'source': 'h04DwdAkNZ4'}),\n",
              " Document(page_content='or in the video description and you can go and try this okay so yeah thank you', metadata={'source': 'h04DwdAkNZ4'})]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can extend this tutorial to create a simple RAG setup using SingleStore as a vector database."
      ],
      "metadata": {
        "id": "gTdTZKGrf5cI"
      }
    }
  ]
}